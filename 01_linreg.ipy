#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Dec 31 17:26:15 2020

@author: townesf
"""

#%%
import numpy as np
from sklearn import linear_model
import statsmodels.api as sm
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers

#%%
rng = np.random.default_rng()
X = rng.normal(loc=5.0,scale=3.0,size=(50,9))
X.mean(0) #should be about 5
X.std(0) #should be about 3

#%%
mu = 33
b = np.array((-3,-1.5,-.5,-.25,-0.01,0.01,.5,1,1.5))
#strong negative: 1,2
#weak negative: 3,4
#insignificant effects: 5,6
#weak positive: 7
#strong positive: 8,9
sigma2 = 4 #fairly high level of noise
y = rng.normal(mu+X@b, np.sqrt(sigma2), size=50)
def mse(y,yhat):
    return ((y-yhat)**2).mean()

#%% Scikit-learn 
fit1 = linear_model.LinearRegression().fit(X,y) #sklearn
mse1 = mse(y,fit1.predict(X))
plt.scatter(b,fit1.coef_)
plt.axline((0,0),slope=1,color='r')

#%% Statsmodels
X1 = sm.tools.tools.add_constant(X)
fit2 = sm.regression.linear_model.OLS(y,X1).fit()
mse2 = mse(y,fit2.predict(X1))
plt.scatter(b,fit2.params[1:])
plt.axline((0,0),slope=1,color='r')

#%% Tensorflow-Keras
tf_model = tf.keras.Sequential([layers.Dense(units=1)])
initial_fit = tf_model.predict(X)
initial_fit.shape
plt.scatter(b,tf_model.layers[0].kernel.numpy().squeeze()) #bad fit, random init
tf_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mean_squared_error')

#%% Tensorflow-Keras
%%time
history1 = tf_model.fit(
    X, y, 
    epochs=1000,
    # suppress logging
    verbose=0)
plt.plot()
fig, ax = plt.subplots()
ax.set_yscale('log')
ax.plot(history1.history['loss'])
ax.axhline(mse1,c='green')
ax.axhline(mse2,c='red',dashes=[5])
plt.show()

#%%
b_keras = tf_model.layers[0].kernel.numpy().squeeze()
plt.scatter(b,b_keras)
plt.axline((0,0),slope=1,color='r')

#%% Tensorflow-Manual
class MyModel(tf.Module):
  def __init__(self,intercept_init, slopes_len, **kwargs):
    super().__init__(**kwargs)
    self.slopes = tf.Variable(rng.normal(size=slopes_len))
    self.intercept = tf.Variable(intercept_init)

  def __call__(self, x):
    return self.intercept+tf.linalg.matvec(x,self.slopes)

def loss(target_y, predicted_y):
  return tf.reduce_mean(tf.square(target_y - predicted_y))
           
SHUFFLE_BUFFER_SIZE = X.shape[0]
BATCH_SIZE = 29
ADAM_LEARN_RATE = 0.1
NUM_EPOCHS = 1000
model = MyModel(y.mean(),X.shape[1])
D = tf.data.Dataset.from_tensor_slices((X, y))
D = D.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
optimizer = tf.optimizers.Adam(ADAM_LEARN_RATE)            
msg = 'Epoch: {:03d}, loss: {:.3E}'
loss_history = []
for epoch in range(NUM_EPOCHS):
    epoch_loss_avg = tf.keras.metrics.Mean()
    for Xb, yb in D: #iterate through each of the batches
        with tf.GradientTape() as tape:
            current_loss = loss(yb, model(Xb))
        grads = tape.gradient(current_loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        epoch_loss_avg.update_state(current_loss)
    loss_history.append(epoch_loss_avg.result())
    if epoch%100==0:
        print(msg.format(epoch,loss_history[epoch]))

#%%
plt.plot()
fig, ax = plt.subplots()
ax.set_yscale('log')
ax.plot(history1.history['loss'],c='green',dashes=[5])
ax.axhline(mse1,c='red')
ax.plot(loss_history)
plt.show()

#%%
plt.scatter(b,model.slopes.numpy())
plt.axline((0,0),slope=1,color='r')

